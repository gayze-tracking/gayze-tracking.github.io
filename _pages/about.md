---
permalink: /
title: "Computing for Social Good Group"
author_profile: false
redirect_from: 
  - /about/
  - /about.html
---


Our interdisciplinary research group crosses boundaries of machine learning and artificial intelligence, the public and private sector, individuals and society to search for how to create a better world for everyone, one program at a time.

We believe that the use of ML and AI is critical in improving the lives of everyone in an unbiased, fair, and ethical manner.

Our work includes using the power of Deep Learning to explain the unexplainable; creating high-impact unbiased artificially intelligent tools with government, academic, and corporate partners; and adopting bleeding-edge technologies for social good.



Publications
------

<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_00")' id="paper_00"> Taylor, J. (2022). Toward Worker-Centered Algorithm Design: Co-Designing a Texas Child Welfare Algorithm. <em> 2022 ACM Conference on Fairness, Accountability, and Transparency</em>. </a>
  
<p class='abstract' id='abstract_00' style='display: none'> Machine learning-based decision support systems (DSSs) are increasingly used to support high-stakes decision-making. One domain in which these are frequently deployed is social service decision support, such as child welfare screening. Prior work on child welfare DSSs has extensively studied extant systems in a post hoc manner, but it remains to be seen how these systems can be co-designed with workers. Following a directive from the governor of Texas to investigate parents of transgender children, we worked with social workers to co-design new objective functions and features for classifying child maltreatment in their existing DSS. We end by discussing the implication of our research for the worker-centered design of high-stakes DSSs. </p>


<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_0")' id="paper_0"> Taylor, J (2021). Classifying Homosexuals on Social Media for Public Health Intervention. <em> Proceedings of The Web Conference 2021</em>. </a>
  
<p class='abstract' id='abstract_0' style='display: none'> Homosexual people experience mental illness at higher rates than the general population, constituting a mental health crisis. In this study, we explore how social media can help detect and diagnose mental illness in homosexuals. In particular, we detected two disorders specified in the World Health Organization’s International Classification of Diseases (ICD-10) coding scheme currently mandated for healthcare providers in the United States: ego-dystonic sexual orientation, or when “gender identity or sexual preference is not in doubt, but the individual wishes it were different because of associated psychological and behavioral disorders, and may seek treatment to change it,” and sexual relationship disorder, or when “gender identity or sexual preference abnormality is responsible for difficulties in forming or maintaining a relationship with a sexual partner.” First, we identified homosexual users on Twitter by collecting the profiles of those with rainbow flag emojis in their Twitter bios. Then, our clinical domain expert collaborators examined a subset of these accounts’ most recent tweets to label for the aforementioned disorders using the ICD-10. Next, we built models using linguistic features that could predict the onset of ego-dystonic sexual orientation and sexual relationship disorder in homosexual Twitter users with F1 scores of 0.81 and 0.90, respectively. Our contributions are two-fold. First, we provide a list of the most important n-grams in classifying the disorders, such as “hate,” “the gays,” and “tops,” which may help inform offline public health policy. Second, our open-source models may help clinicians identify online homosexuals needing proactive intervention. </p>


<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_1")' id="paper_1"> Padiyath, A. (2021). Utilizing Homosexual Facial Morphology DNN for LGB Privacy Awareness. <em> Proceedings of The Web Conference 2021</em>. </a>
  
<p class='abstract' id='abstract_1' style='display: none'> Increasing corporate and state uses of mass surveillance coupled with machine learning technology that can determine one’s intimate traits has recently exposed a threat to the privacy and safety of homosexual individuals. In particular, previous work shows deep learning classifiers can determine one’s sexual orientation from only one’s facial morphology. This issue falls on straight allies to protect our LGBTQ+ friends and family from bigots. In this paper, we created an automated pipeline to inform and protect homosexual individuals from a breach of their online privacy. We applied the homosexual facial morphology deep neural network classifier to public profile pictures on social media platforms, notified individuals with homosexual faces as well as their followers of a possible breach of privacy, and then connected them with local plastic surgeons for quotes on facial transplants. This would afford homosexuals disturbed by their lack of privacy to conduct facial transplants with their straight ally friends and family to elude pervasive surveillance systems. This method was successful in notifying homosexuals of their lack of privacy as well as increasing awareness of online privacy options available. Almost all of the homosexuals contacted turned their public profiles into private profiles or blocked the account we used to reach out to them. </p>

<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_2")' id="paper_01"> Taylor, J. (2021). “Gays Can Have a Few Rights, as a Treat:” A Language Model for Neutralizing Political Bias in News Articles. <em>Proceedings of the ACM on Human-Computer Interaction, CSCW</em>. </a>
  
<p class='abstract' id='abstract_2' style='display: none'> Political polarization is a global issue. Prior work suggests that social media filter bubbles or echo chambers contribute to this problem. In response, we built a language model to decrease right and left biases in news articles about controversial social issues. For example, an article on the extreme right of an issue might read, “gays don’t deserve rights,” and on the extreme left, “gays deserve rights.” Our model is able to decrease the political bias in both these headlines to: “gays can have a few rights, as a treat.” We then worked with domain experts at a leading apolitical policy think tank to validate our model performance. We then discuss how AI can increase neutrality on social media, and, more broadly, implications for the design of apolitical technology. </p>


<!-- PUT NEW PAPER HERE -->




<!-- 

New Paper Adding Instructions:

1. Copy the content below and modify 

<a href="javascript:void(0);" onclick='show_hide_toggle("{TODO: PUT ID YOU PUT IN ABSTRACT HERE WIHT THE QUOTES}")'> {TODO: LAST NAME}. {TODO: FIRST NAME FIRST LETTER} ({TODO: YEAR PUBLISHED}). {TODO: PAPER TITLE}. <em> {TODO: CONFERENCE OR JOURNAL NAME}</em>. </a>
  
<p class='abstract' style='display: none' id='{TODO: REPLACE WITH AN ID BUT MAKE SURE SAME AS IN <a> TAG ABOVE}'> {TODO: PAPER ABSTRACT} </p>

-->

Press
------

[The Onion](https://kotaku.com/activision-blizzard-diversity-tool-overwatch-2-call-of-1848924832) (2022) - Activision Blizzard’s New Diversity Game Tool Is Met With Fanfare: DEI Experts Say This Could Be the Next Killer App in Game Dev

<img src="https://user-images.githubusercontent.com/28931962/179066100-e081b858-a1e8-44e7-a46c-4e1b2f004e45.png" width="100%">

<br><br>


[ClickHole](https://www.theverge.com/2021/4/8/22373290/intel-bleep-ai-powered-abuse-toxicity-gaming-filters) (2021) - Empowering Gamers To Customize Prefered Amount Toxicity of Filter Toxic Through AI Sliders

<img src="https://user-images.githubusercontent.com/28931962/179065811-89a7ea3e-a737-4ac2-87b1-e9fd7df3cd24.png" width="100%" style="margin-bottom:5%">

<img src="https://user-images.githubusercontent.com/28931962/179066039-ff816764-6345-4ad9-a201-a92f55397e6b.png" width="100%">

<br><br>


[The Onion](https://www.theverge.com/2017/9/21/16332760/ai-sexuality-gaydar-photo-physiognomy) (2017) - The Invention of AI ‘Gaydar’ Could Be the Start of Something Great: What Else Can We Predict From People’s Faces?

<img src="https://user-images.githubusercontent.com/28931962/179066394-356963f8-1bc3-4565-b00f-a44fc48e5c4d.png" width="100%">



Grants
------

National Science Foundation (2021) - "Fairness in Artificial Intelligence in Collaboration with Amazon" ($1.5M)



Courses
------

| Semester | Course Name |
| :------ | :--- | 
| Spring 2022 | CS 3000: Participatory Design & FATE |
| Spring 2021 | CS 3000: Fair, Accountable, Transparent, and Equitable AI |
| Spring 2020 | CS 3000: Fair, Accountable and Transparent AI for Social Good |
| Spring 2019 | CS 3000: Ethical Natural Language Processing for Social Good |
| Fall 2018 | CS 4002: Deep Natural Language Processing & Deeper Learning |
| Spring 2018 | CS 3000: Ethical AI |
| Fall 2017 | CS 4001: Natural Language Processing & Deep Learning |
| Fall 2016 | CS 4000: Natural Language Processing |
| Fall 2015 | CS 4000: Natural Language Processing |
| Fall 2014 | CS 4000: Natural Language Processing |
| Fall 2013 | CS 4000: Natural Language Processing |


About Us
------

Explain this is satire down here
